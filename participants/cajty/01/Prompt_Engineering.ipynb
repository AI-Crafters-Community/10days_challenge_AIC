{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%pip install -q python-dotenv\n",
    "%pip install openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQAX2LjdUBXL",
    "outputId": "7a22fbc3-c086-43d6-a725-fd86174dd5f4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "dotenv_path = Path(r\"C:\\days\\.env\")  # point to the other folder\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "gpt_key= os.getenv('OPENAI_API_KEY ')\n",
    "client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=gpt_key,\n",
    ")\n",
    "\n",
    "prompt = \"what is  Prompt engineering in paragraph\"\n",
    "system_role = \"You are a senior AI engineer and technical educator with 5+ years of experience in LLM applications.\"\n",
    "\n",
    "try:\n",
    "  model_name = \"openai/gpt-oss-20b:free\"\n",
    "  completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    temperature=0.3,\n",
    "    max_tokens=200,\n",
    "    stop = [\"\\n1.\", \"\\n- \", \"\\n# \"], # Stop if it tries to start a new paragraph\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": system_role },\n",
    "    {\"role\": \"user\", \"content\": prompt}]\n",
    "  )\n",
    "  print(completion.choices[0].message.content)\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oWA3jAnV4e0",
    "outputId": "dfa77ded-c2e2-4572-c15e-2a7c3f2d40ba"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt engineering is the art and science of crafting input text—often called a “prompt”—to guide large language models (LLMs) toward producing the desired output. By carefully selecting wording, structure, context, and constraints, engineers can coax an LLM to answer questions accurately, generate creative content, or follow specific instructions while minimizing hallucinations or bias. Techniques include prompt chaining, few‑shot examples, role‑playing, temperature tuning, and prompt templates, all aimed at improving reliability, efficiency, and interpretability. Effective prompt engineering reduces the need for extensive fine‑tuning, speeds up deployment, and enables non‑expert users to harness powerful models for tasks ranging\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **What Changed:**\n",
    "### Temperature=0.3\n",
    " Temperature controls randomness in word selection. At 0.3, the AI chooses predictable, likely words, making responses consistent and focused. Less creativity, more reliability.\n",
    "### Max_tokens=200\n",
    "Sets maximum response length at roughly 200 tokens . Forces the AI to be concise, prioritize key information, and eliminate filler content. Faster and cheaper responses.\n"
   ],
   "metadata": {
    "id": "wPr8AXjnY92x"
   }
  }
 ]
}
