{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe058aa2"
      },
      "source": [
        "# Prompt Engineering Guide\n",
        "Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases.\n",
        "\n",
        "**Prompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering to improve safety and the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.**\n",
        "\n",
        "This comprehensive guide covers the theory and practical aspects of prompt engineering and how to leverage the best prompting techniques to interact and build with LLMs.\n",
        "\n",
        "This guide will cover:\n",
        "\n",
        "*   Basic prompt structure\n",
        "*   Techniques for improving prompt effectiveness\n",
        "*   Examples of prompt engineering for different tasks\n",
        "*   Tips for debugging and refining prompts\n",
        "\n",
        "## Basic Prompt Structure\n",
        "\n",
        "A basic prompt often includes:\n",
        "\n",
        "1.  **Instruction:** What you want the model to do.\n",
        "2.  **Context (Optional):** Any background information the model needs.\n",
        "3.  **Input Data (Optional):** The specific data the model should process.\n",
        "4.  **Output Indicator (Optional):** How you want the output to be formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcLKRgbzijL6",
        "outputId": "ddab062c-aeec-4ae8-cac3-8f4355a81c2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%echo` not found.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q python-dotenv\n",
        "%pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "SVHQNHh8ijL7",
        "outputId": "431d72df-6a84-4f32-eec3-1f5d78557fc8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b44220af-634d-499f-8985-35203c9e4e88\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b44220af-634d-499f-8985-35203c9e4e88\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .env to .env\n",
            "sk-or...\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Go up one directory (from /days/01 → /days) and load .env\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# You will be prompted to upload .env file contaning your openai api key\n",
        "uploaded = files.upload()\n",
        "\n",
        "load_dotenv(\".env\")\n",
        "\n",
        "# Now you can use the key\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "print(api_key[:5] + \"...\" if api_key else \"API key not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bcedb8c",
        "outputId": "4ba4c14c-803c-4c29-9c6c-0021d589f2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM:\n",
            "analysisWe need to write a short story about a dog who loves to read. It's a whimsical, likely cute story. We need to imagine a dog who loves reading. The dog could be a literature-loving dog, maybe with a library. The dog could be anthropomorphic or just a real dog with a love for books. The story could explore the dog's nature, its interactions with humans, perhaps the dog reading by itself, or reading to humans. The dog might have a special library, or a favorite book. The story could be a bit magical. We should write in a narrative style, maybe third person, but with rich details. The story should be short but complete, with a beginning, middle, end. Perhaps the dog is named \"Barkley\" or \"Page\" or \"Scribe.\" The dog might be in a small town or a library. The story could be about how the dog learns to read, or how the dog loves the smell of paper, the sound of pages turning. The dog might be reading \"The Adventures of Sherlock Bones\" or \"Woof.\" The dog could also have a love for poetry. The dog could influence others.\n",
            "\n",
            "We can use a whimsical tone. The dog could be a therapy dog at a library, reading to children. The story could involve the dog encountering a lonely child who loves books, and they bond. Or a dog reading to a lonely old man. Or the dog reading to the pups.\n",
            "\n",
            "Alternatively, we could write from the dog's perspective, giving the dog a voice. The dog could describe the sensation of books. The dog might learn to read with help of a kind human.\n",
            "\n",
            "We can have the dog read a particular book that changes its life, like \"The Dog's Guide to Life\" or \"The Other Side of the Book.\" The story could end with the dog reading to a new generation of dogs, or maybe the dog having a library. The dog might think of reading as an adventure.\n",
            "\n",
            "Ok, we need to write a short story. Let's choose a dog named \"Pip\" or \"Sage.\" Let's create a narrative. Maybe the dog is a small terrier in a town library. The dog loves to sit by the window, read. The dog could be a part of the library's \"Reading Program.\" The dog might have a special book: \"The Secret of the Stacks.\" The dog might help the librarian find lost books.\n",
            "\n",
            "We need to be careful to keep it short but satisfying. Let's write about a dog named \"Milo\" who loves reading. He lives in a cottage with a big library. He reads in the window. He has a habit of reading out loud. He has a favorite book, perhaps \"The Adventures of Paw-some.\" He helps the children in the town. He also learns about the power of words. The story ends with him reading to a new dog.\n",
            "\n",
            "Alternatively, we could write a more magical story: dog is a librarian in a secret library for animals. He reads to all the animals. The dog loves reading, because reading is a portal. The dog helps a lost kitten find home.\n",
            "\n",
            "We can incorporate the dog's senses: love of the smell of paper, the sound of pages. He thinks of words as chewable bones.\n",
            "\n",
            "Ok, let's start. We'll set the scene: a small town library, a dog named \"Milo.\" He loves reading. He has a particular habit. The story will show his interactions with the librarian, kids, and his own reading.\n",
            "\n",
            "We'll write in third person, present tense maybe.\n",
            "\n",
            "We should also include a twist: maybe the dog reads a book that reveals a secret. Or the dog is the librarian's companion. He is the library's \"Dog of the Month.\" He is beloved.\n",
            "\n",
            "Alternatively, we can write from dog's perspective, which might be tender.\n",
            "\n",
            "Alright, let's draft.\n",
            "\n",
            "Title: \"The Bookish Dog\"\n",
            "\n",
            "Narrative:\n",
            "\n",
            "Milo is a small terrier with a big heart and a bigger nose. He lives with Mrs. Whitaker in a cottage with a library.\n",
            "\n",
            "Milo's love for books is unusual. He hears the rustle of pages and smells the ink. He sits at the window, nose in a book.\n",
            "\n",
            "One day, a new child named Lily arrives. She is shy and doesn't like to read. Milo hears her sighing, and he decides to help. He brings a book to her.\n",
            "\n",
            "Milo's reading voice is gentle. He reads the book aloud, his bark becomes a rhythm. He encourages Lily to open the book.\n",
            "\n",
            "Through the story, Milo learns that reading isn't just about words, but about imagination. He becomes a bridge.\n",
            "\n",
            "At the end, Milo leaves a book on Lily's table, with a note: \"For you, with paws.\"\n",
            "\n",
            "Ok, we can write that.\n",
            "\n",
            "Alternatively, we can incorporate the dog's own reading ability: he can read in a way. He has a book of his own, maybe a dog treat recipe.\n",
            "\n",
            "We need to end with a gentle note about the power of reading.\n",
            "\n",
            "Alright, let's write. We'll include a bit of humor: the dog bows his head, the book falls.\n",
            "\n",
            "Let's write a short story with around \n",
            "\n",
            "Full chat_completion object:\n",
            "ChatCompletion(id='gen-1756838370-qAIbaZTyakKpLWplwoAn', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='analysisWe need to write a short story about a dog who loves to read. It\\'s a whimsical, likely cute story. We need to imagine a dog who loves reading. The dog could be a literature-loving dog, maybe with a library. The dog could be anthropomorphic or just a real dog with a love for books. The story could explore the dog\\'s nature, its interactions with humans, perhaps the dog reading by itself, or reading to humans. The dog might have a special library, or a favorite book. The story could be a bit magical. We should write in a narrative style, maybe third person, but with rich details. The story should be short but complete, with a beginning, middle, end. Perhaps the dog is named \"Barkley\" or \"Page\" or \"Scribe.\" The dog might be in a small town or a library. The story could be about how the dog learns to read, or how the dog loves the smell of paper, the sound of pages turning. The dog might be reading \"The Adventures of Sherlock Bones\" or \"Woof.\" The dog could also have a love for poetry. The dog could influence others.\\n\\nWe can use a whimsical tone. The dog could be a therapy dog at a library, reading to children. The story could involve the dog encountering a lonely child who loves books, and they bond. Or a dog reading to a lonely old man. Or the dog reading to the pups.\\n\\nAlternatively, we could write from the dog\\'s perspective, giving the dog a voice. The dog could describe the sensation of books. The dog might learn to read with help of a kind human.\\n\\nWe can have the dog read a particular book that changes its life, like \"The Dog\\'s Guide to Life\" or \"The Other Side of the Book.\" The story could end with the dog reading to a new generation of dogs, or maybe the dog having a library. The dog might think of reading as an adventure.\\n\\nOk, we need to write a short story. Let\\'s choose a dog named \"Pip\" or \"Sage.\" Let\\'s create a narrative. Maybe the dog is a small terrier in a town library. The dog loves to sit by the window, read. The dog could be a part of the library\\'s \"Reading Program.\" The dog might have a special book: \"The Secret of the Stacks.\" The dog might help the librarian find lost books.\\n\\nWe need to be careful to keep it short but satisfying. Let\\'s write about a dog named \"Milo\" who loves reading. He lives in a cottage with a big library. He reads in the window. He has a habit of reading out loud. He has a favorite book, perhaps \"The Adventures of Paw-some.\" He helps the children in the town. He also learns about the power of words. The story ends with him reading to a new dog.\\n\\nAlternatively, we could write a more magical story: dog is a librarian in a secret library for animals. He reads to all the animals. The dog loves reading, because reading is a portal. The dog helps a lost kitten find home.\\n\\nWe can incorporate the dog\\'s senses: love of the smell of paper, the sound of pages. He thinks of words as chewable bones.\\n\\nOk, let\\'s start. We\\'ll set the scene: a small town library, a dog named \"Milo.\" He loves reading. He has a particular habit. The story will show his interactions with the librarian, kids, and his own reading.\\n\\nWe\\'ll write in third person, present tense maybe.\\n\\nWe should also include a twist: maybe the dog reads a book that reveals a secret. Or the dog is the librarian\\'s companion. He is the library\\'s \"Dog of the Month.\" He is beloved.\\n\\nAlternatively, we can write from dog\\'s perspective, which might be tender.\\n\\nAlright, let\\'s draft.\\n\\nTitle: \"The Bookish Dog\"\\n\\nNarrative:\\n\\nMilo is a small terrier with a big heart and a bigger nose. He lives with Mrs. Whitaker in a cottage with a library.\\n\\nMilo\\'s love for books is unusual. He hears the rustle of pages and smells the ink. He sits at the window, nose in a book.\\n\\nOne day, a new child named Lily arrives. She is shy and doesn\\'t like to read. Milo hears her sighing, and he decides to help. He brings a book to her.\\n\\nMilo\\'s reading voice is gentle. He reads the book aloud, his bark becomes a rhythm. He encourages Lily to open the book.\\n\\nThrough the story, Milo learns that reading isn\\'t just about words, but about imagination. He becomes a bridge.\\n\\nAt the end, Milo leaves a book on Lily\\'s table, with a note: \"For you, with paws.\"\\n\\nOk, we can write that.\\n\\nAlternatively, we can incorporate the dog\\'s own reading ability: he can read in a way. He has a book of his own, maybe a dog treat recipe.\\n\\nWe need to end with a gentle note about the power of reading.\\n\\nAlright, let\\'s write. We\\'ll include a bit of humor: the dog bows his head, the book falls.\\n\\nLet\\'s write a short story with around ', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='length')], created=1756838370, model='openai/gpt-oss-20b:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=1050, prompt_tokens=79, total_tokens=1129, completion_tokens_details=None, prompt_tokens_details=None), provider='AtlasCloud')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "# It's recommended to store your API key securely, for example, in Colab Secrets\n",
        "#from google.colab import userdata\n",
        "#gpt_key= userdata.get('gpt')\n",
        "\n",
        "load_dotenv(\".env\", override=True)\n",
        "\n",
        "gpt_key= os.getenv('OPENAI_API_KEY ')\n",
        "client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=gpt_key,\n",
        ")\n",
        "\n",
        "# Example prompt\n",
        "prompt_text = \"Write a short story about a dog who loves to read.\"\n",
        "\n",
        "# Example API call (using a model available on OpenRouter)\n",
        "# You can find available models and their names on the OpenRouter website\n",
        "model_name = \"openai/gpt-oss-20b:free\" # Example model, choose one from OpenRouter\n",
        "print(f\"Attempting to call model: {model_name}\")\n",
        "\n",
        "try:\n",
        "  chat_completion = client.chat.completions.create(\n",
        "  model=model_name,\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "  temperature=0.7,\n",
        "  max_tokens=1050,\n",
        "        )\n",
        "  print(\"Response from LLM:\")\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "  # Optionally, print the full object for debugging:\n",
        "  print(\"\\nFull chat_completion object:\")\n",
        "  print(chat_completion)\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during the API call: {e}\")\n",
        "  print(f\"Please ensure your API key is correct and the model '{model_name}' is valid and available on OpenRouter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G7EVmybsLSE",
        "outputId": "013bb923-1589-4cc8-be72-471e2c954b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The question “What is the meaning of life?” has fascinated philosophers, scientists, artists, and ordinary people for millennia. Because it touches on the deepest parts of our existence—our purpose, our values, the significance of our experiences—there is no single answer that satisfies everyone. Instead, most thinkers agree that the meaning of life is something we create, discover, or interpret rather than uncover.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. A Personal Canvas: Constructing Your Own Meaning\n",
            "\n",
            "### *Existentialist View*\n",
            "Existentialists like Jean Paul Sartre and Albert Camus argue that life is inherently meaningless, but that freedom gives us the responsibility to give it meaning. In this view, your purpose is self‑authored: the projects you choose, the relationships you nurture, the passions you pursue—all of these become the “meaning” of your life.\n",
            "\n",
            "### *Humanistic Psychology*\n",
            "Carl Rogers and Abraham Maslow suggest that meaning arises from growth, self‑actualization, and authenticity. When you align your actions with your deepest values and potentials, you experience a sense of purpose.\n",
            "\n",
            "### *Practical Takeaway*\n",
            "- **Ask**: What brings you joy, fulfillment, or a sense of contribution?\n",
            "- **Experiment**: Try new activities, volunteer, learn, or create. Notice what feels “right.”\n",
            "- **Reflect**: Keep a journal or discuss with trusted friends. Over time you’ll see patterns that point to your own meaning.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. A Shared Human Story: Collective Perspectives\n",
            "\n",
            "### *Religious and Spiritual Traditions*\n",
            "Most world religions propose that life’s meaning comes from a relationship with the divine or a cosmic order. For example:\n",
            "- **Christianity**: Love God and others; live according to divine commandments.\n",
            "- **Buddhism**: Seek enlightenment and reduce suffering through mindfulness and compassion.\n",
            "- **Islam**: Serve Allah, uphold justice, and live a virtuous life.\n",
            "\n",
            "### *Secular Ethics*\n",
            "Philosophers like John Rawls or Peter Singer argue that moral duties—fairness, compassion, stewardship—give life purpose. The meaning is found in acting for the common good and respecting others’ autonomy.\n",
            "\n",
            "### *Scientific Lens*\n",
            "From a biological standpoint, the meaning could be as simple as survival and reproduction. Yet many scientists see meaning in the quest for knowledge, curiosity, and the understanding of the universe.\n",
            "\n",
            "### *Practical Takeaway*\n",
            "- **Explore**: Read texts, attend services, or study philosophies that resonate with you.\n",
            "- **Integrate**: Combine insights from different traditions if they add value.\n",
            "- **Live**: Apply principles that align with your values, whether they’re religious, ethical, or scientific.\n",
            "\n",
            "---\n",
            "\n",
            "## 3. The “Universal” Threads Often Found in Meaningful Lives\n",
            "\n",
            "| Thread | Why It Matters | How It Shows Up |\n",
            "|--------|----------------|-----------------|\n",
            "| **Connection** | Humans are social beings; relationships give context to our actions. | Family, friendships, community service. |\n",
            "| **Growth** | Learning and development keep life dynamic. | Education, skill‑building, personal reflection. |\n",
            "| **Contribution** | Making a difference fosters a sense of purpose. | Volunteering, creative work, mentoring. |\n",
            "| **Passion** | Pursuing what excites you fuels energy and resilience. | Hobbies, career choices, causes you care about. |\n",
            "| **Mindfulness** | Staying present reduces anxiety and enhances appreciation. | Meditation, journaling, slow living. |\n",
            "\n",
            "---\n",
            "\n",
            "## 4. A Simple Experiment: “The Meaning Map”\n",
            "\n",
            "1. **List 3 activities that make you feel alive.**  \n",
            "2. **Ask each: “Why does this matter to me?”**  \n",
            "3. **Identify common values that emerge.**  \n",
            "4. **Sketch a rough “meaning map” linking activities → values → impact.**\n",
            "\n",
            "This visual can help you see connections you might miss in conversation.\n",
            "\n",
            "---\n",
            "\n",
            "## 5. A Final Thought\n",
            "\n",
            "Many people find that the search itself becomes part of life’s meaning. When we ask, we engage our curiosity, we confront uncertainty, and we become active participants in our own story. Whether you lean toward a grand cosmic purpose or a simple personal fulfillment, the act of seeking—and living with intention—makes life profoundly meaningful.\n",
            "\n",
            "---\n",
            "\n",
            "### Takeaway\n",
            "\n",
            "- **Meaning is personal yet universal in its core threads.**  \n",
            "- **It can be discovered, created, or interpreted.**  \n",
            "- **Reflect, experiment, and act in alignment with your values.**  \n",
            "\n",
            "If you’d like to explore a specific tradition, philosophy, or activity\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=gpt_key,\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  extra_body={},\n",
        "  model=\"openai/gpt-oss-20b:free\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=1000,\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What is the meaning of life?\"\n",
        "    }\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFp1zhgyHbwR"
      },
      "source": [
        "## LLM Settings\n",
        "* **Temperature** → controls randomness of next-token choice.\n",
        "\n",
        "  * Lower (0–0.3): more deterministic, concise (good for fact Q&A).\n",
        "\n",
        "  * Higher (0.7–1.0+): more varied/creative (good for poems/brainstorm).\n",
        "\n",
        "* **Top-p** (nucleus sampling) → limits choices to the smallest set whose probabilities sum to p.\n",
        "\n",
        "  * Lower (0.1–0.3): very focused, conservative.\n",
        "\n",
        "  * Higher (0.9–1.0): considers more (rarer) words → more diversity.\n",
        "\n",
        "      * Tip: tune either temperature or top-p, not both.\n",
        "\n",
        "* **Max length** (max tokens) → hard cap on output size.\n",
        "\n",
        "  * Prevents run-on/irrelevant answers and manages cost.\n",
        "\n",
        "  * Stop sequences → strings that make the model stop generating.\n",
        "\n",
        "  * Use to enforce structure/length (e.g., stop at \"11\" to cap a numbered list at 10).\n",
        "\n",
        "* **Frequency penalty** → reduces reuse proportional to how often a token already appeared.\n",
        "\n",
        "  * Higher value = fewer repeated words.\n",
        "\n",
        "  * Presence penalty → discourages any repeated token equally (once it’s appeared).\n",
        "\n",
        "  * Higher value = less phrase repetition; boosts topical variety.\n",
        "\n",
        "  * Use higher for exploration/creativity; lower to keep the model tightly on topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "319fba6b"
      },
      "source": [
        "## Prompt Examples\n",
        "Here are examples of Zero-shot, One-shot, and Few-shot prompting techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iqh59uCEFPs"
      },
      "source": [
        "### Zero-shot Prompting\n",
        "\n",
        "Zero-shot prompting is when you give the model a task without providing any examples of how to do it. The model relies solely on its pre-training to understand and complete the task.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23e5bebd",
        "outputId": "3570caf8-5080-4e17-af93-e0a3d05468df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot Prompt:\n",
            "Classify the following text as positive, negative, or neutral: 'I love this new movie!'\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot Prompt Example\n",
        "zero_shot_prompt = \"Classify the following text as positive, negative, or neutral: 'I love this new movie!'\"\n",
        "print(f\"Zero-shot Prompt:\\n{zero_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLCNuCD0Dk1L"
      },
      "source": [
        "### One-shot Prompting\n",
        "One-shot prompting is when you provide the model with one example of the task you want it to perform, along with the desired output for that example. This helps the model understand the format and style you're looking for in its response to your actual query.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6136412",
        "outputId": "a3a68696-80bf-43fa-cd43-fea46bdb5240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-shot Prompt:\n",
            "Given the following example, classify the next text.\n",
            "\n",
            "Example:\n",
            "Text: 'This is a terrible product.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'The weather is nice today.'\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "# One-shot Prompt Example\n",
        "one_shot_prompt = \"\"\"Given the following example, classify the next text.\n",
        "\n",
        "Example:\n",
        "Text: 'This is a terrible product.'\n",
        "Sentiment: Negative\n",
        "\n",
        "Classify the following text:\n",
        "Text: 'The weather is nice today.'\n",
        "Sentiment:\"\"\"\n",
        "print(f\"One-shot Prompt:\\n{one_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycp2jX3yDvzm"
      },
      "source": [
        "### Few-shot prompting\n",
        "Few-shot prompting is similar to one-shot prompting, but instead of just one example, you provide the model with a few examples (typically between 2 and 5) of the task and their corresponding outputs. This gives the model more context and helps it to better grasp the pattern or logic required to complete the task accurately.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b58601f",
        "outputId": "b265be45-d387-404b-f3b2-c7ed216faa2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-shot Prompt:\n",
            "Given the following examples, classify the next text.\n",
            "\n",
            "Example 1:\n",
            "Text: 'I had a wonderful time.'\n",
            "Sentiment: Positive\n",
            "\n",
            "Example 2:\n",
            "Text: 'The service was slow.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Example 3:\n",
            "Text: 'It was an average experience.'\n",
            "Sentiment: Neutral\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'I would recommend this restaurant.'\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "# Few-shot Prompt Example\n",
        "few_shot_prompt = \"\"\"Given the following examples, classify the next text.\n",
        "\n",
        "Example 1:\n",
        "Text: 'I had a wonderful time.'\n",
        "Sentiment: Positive\n",
        "\n",
        "Example 2:\n",
        "Text: 'The service was slow.'\n",
        "Sentiment: Negative\n",
        "\n",
        "Example 3:\n",
        "Text: 'It was an average experience.'\n",
        "Sentiment: Neutral\n",
        "\n",
        "Classify the following text:\n",
        "Text: 'I would recommend this restaurant.'\n",
        "Sentiment:\"\"\"\n",
        "print(f\"Few-shot Prompt:\\n{few_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Xhv8cIEh_N",
        "outputId": "66fa87cf-f579-4bd2-e516-4e18f79aedb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sending Zero-shot Prompt ---\n",
            "Prompt:\n",
            "Classify the following text as positive, negative, or neutral: 'I love this new movie!'\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (Zero-shot):\n",
            "analysisWe need to classify sentiment. The example: \"I love this new movie!\" is positive. So answer: positive.assistantfinalPositive\n",
            "\n",
            "--- Sending One-shot Prompt ---\n",
            "Prompt:\n",
            "Given the following example, classify the next text.\n",
            "\n",
            "Example:\n",
            "Text: 'This is a terrible product.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'The weather is nice today.'\n",
            "Sentiment:\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (One-shot):\n",
            "analysisWe have a task: Sentiment analysis. Example: \"This is a terrible product.\" => Negative. Then classify: \"The weather is nice today.\" Likely Positive sentiment. So answer: Positive.assistantfinalSentiment: Positive\n",
            "\n",
            "--- Sending Few-shot Prompt ---\n",
            "Prompt:\n",
            "Given the following examples, classify the next text.\n",
            "\n",
            "Example 1:\n",
            "Text: 'I had a wonderful time.'\n",
            "Sentiment: Positive\n",
            "\n",
            "Example 2:\n",
            "Text: 'The service was slow.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Example 3:\n",
            "Text: 'It was an average experience.'\n",
            "Sentiment: Neutral\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'I would recommend this restaurant.'\n",
            "Sentiment:\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (Few-shot):\n",
            "analysisWe need to determine the sentiment of the text \"I would recommend this restaurant.\" This is positive. So sentiment: Positive. Provide answer.assistantfinalSentiment: Positive\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'client' is already defined and configured from a previous cell\n",
        "# and 'zero_shot_prompt', 'one_shot_prompt', and 'few_shot_prompt' are defined\n",
        "\n",
        "few_shot_prompt = \"\"\"Given the following examples, classify the next text.\n",
        "\n",
        "Example 1:\n",
        "Text: 'I had a wonderful time.'\n",
        "Sentiment: Positive\n",
        "\n",
        "Example 2:\n",
        "Text: 'The service was slow.'\n",
        "Sentiment: Negative\n",
        "\n",
        "Example 3:\n",
        "Text: 'It was an average experience.'\n",
        "Sentiment: Neutral\n",
        "\n",
        "Classify the following text:\n",
        "Text: 'I would recommend this restaurant.'\n",
        "Sentiment:\"\"\"\n",
        "prompts = {\n",
        "    \"Zero-shot\": zero_shot_prompt,\n",
        "    \"One-shot\": one_shot_prompt,\n",
        "    \"Few-shot\": few_shot_prompt\n",
        "}\n",
        "\n",
        "for prompt_type, prompt_text in prompts.items():\n",
        "  print(f\"\\n--- Sending {prompt_type} Prompt ---\")\n",
        "  print(f\"Prompt:\\n{prompt_text}\")\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    model=\"openai/gpt-oss-20b:free\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1050,\n",
        "          )\n",
        "    print(f\"Response from LLM ({prompt_type}):\")\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "    # Optionally, print the full object for debugging:\n",
        "    # print(f\"\\nFull chat_completion object ({prompt_type}):\")\n",
        "    # print(chat_completion)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during the API call for {prompt_type} prompt: {e}\")\n",
        "    print(f\"Please ensure your API key is correct and the model '{model_name}' is valid and available on OpenRouter.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b66a625"
      },
      "source": [
        "## Chain-of-Thought Prompting\n",
        "\n",
        "Chain-of-Thought (CoT) prompting is a technique that encourages the language model to explain its reasoning process step-by-step before arriving at the final answer. This can lead to more accurate and reliable results, especially for complex tasks, as it allows the model to break down the problem and work through it logically.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "By adding phrases like \"Let's think step by step\" or explicitly asking for intermediate steps, you guide the model to generate a series of thoughts or reasoning steps that lead to the solution.\n",
        "\n",
        "**Benefits of CoT:**\n",
        "\n",
        "* **Improved accuracy:** Breaking down complex problems into smaller steps reduces the chance of errors.\n",
        "* **Increased transparency:** You can see how the model arrived at its answer, making it easier to debug or understand its limitations.\n",
        "* **Better performance on complex tasks:** CoT has shown significant improvements on tasks requiring multi-step reasoning, like arithmetic or symbolic manipulation.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af5d430e",
        "outputId": "df1aa4b4-bae6-4828-af3d-d3e41d4c034a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain-of-Thought Prompt:\n",
            "The original price of a shirt was $20. It was discounted by 25%, and then an additional 10% discount was applied to the sale price. What is the final price of the shirt?\n",
            "\n",
            "Let's think step by step.\n"
          ]
        }
      ],
      "source": [
        "# Chain-of-Thought Prompt Example\n",
        "cot_prompt = \"\"\"The original price of a shirt was $20. It was discounted by 25%, and then an additional 10% discount was applied to the sale price. What is the final price of the shirt?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "print(f\"Chain-of-Thought Prompt:\\n{cot_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w_3bGmrijMF",
        "outputId": "20993bf6-1e43-4c0c-dd58-c8c81ef1d479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response (Chain-of-Thought):\n",
            "\n",
            "analysisWe need to compute final price after successive discounts: first 25% off $20, then 10% off new price. So 25% of 20 = 5, so price after first discount = 20 - 5 = 15. Then 10% off 15 = 1.5, so final price = 15 - 1.5 = 13.5. So answer $13.50. Provide step-by-step explanation.assistantfinal**Step 1 – First discount (25 %)**  \n",
            "- Original price: \\$20  \n",
            "- Discount amount: \\(25\\% \\times 20 = 0.25 \\times 20 = \\$5\\)  \n",
            "- Price after first discount: \\(20 - 5 = \\$15\\)\n",
            "\n",
            "**Step 2 – Second discount (10 %)**  \n",
            "- New price before second discount: \\$15  \n",
            "- Discount amount: \\(10\\% \\times 15 = 0.10 \\times 15 = \\$1.50\\)  \n",
            "- Final price: \\(15 - 1.50 = \\$13.50\\)\n",
            "\n",
            "\\[\n",
            "\\boxed{\\$13.50}\n",
            "\\]\n"
          ]
        }
      ],
      "source": [
        "# Send the Chain-of-Thought prompt to the LLM and print the response\n",
        "cot_prompt = \"\"\"The original price of a shirt was $20. It was discounted by 25%, and then an additional 10% discount was applied to the sale price. What is the final price of the shirt?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "  chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0.2,\n",
        "    max_tokens=300\n",
        "  )\n",
        "  print(\"Response (Chain-of-Thought):\\n\")\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during the API call: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6LbV7rsijMF"
      },
      "source": [
        "## System / Role Prompting\n",
        "\n",
        "You can steer behavior with explicit roles:\n",
        "\n",
        "- **system**: high-level instructions and persona\n",
        "- **user**: task request\n",
        "- **assistant**: optional exemplars or tools output\n",
        "\n",
        "This helps keep outputs consistent, especially for style and safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK4G4wLUijMF",
        "outputId": "a8f91e4b-46e2-42b1-c9f8-9a617ae1e656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response (System/Role Prompting):\n",
            "\n",
            "analysisWe need to answer in bullet points, each under 12 words. Concise. Explain what embeddings are and one practical use-case. So maybe 2-3 bullets. Each bullet under 12 words. Let's do:\n",
            "\n",
            "- Embeddings: numeric vectors representing data semantics. \n",
            "- Use-case: document similarity search in knowledge bases. \n",
            "- They enable machine learning models to process text efficiently.\n",
            "\n",
            "We need each bullet under 12 words. Count words:\n",
            "\n",
            "Bullet 1: \"Embeddings: numeric vectors representing data semantics.\" Words: Embeddings(1): numeric(2) vectors(3) representing(4) data(5) semantics(6). That's 6 words. Good.\n",
            "\n",
            "Bullet 2: \"Use-case: document similarity search in knowledge bases.\" Words: Use-case(1): document(2) similarity(3) search(4) in(5) knowledge(6) bases(7). 7 words.\n",
            "\n",
            "Bullet 3: \"They enable machine\n"
          ]
        }
      ],
      "source": [
        "# System / Role Prompting demo\n",
        "system_msg = (\n",
        "  \"You are a concise technical writer. Answer in bullet points, each under 12 words.\"\n",
        ")\n",
        "user_msg = \"Explain what embeddings are and one practical use-case.\"\n",
        "\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    temperature=0.4,\n",
        "    max_tokens=200,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": system_msg},\n",
        "      {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "  )\n",
        "  print(\"Response (System/Role Prompting):\\n\")\n",
        "  print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sII9LjYOijMG"
      },
      "source": [
        "## Stop Sequences\n",
        "Use stop strings to end generation at desired boundaries (e.g., JSON blocks, section breaks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2iO7DTFijMG",
        "outputId": "fc5249b3-5d6b-4163-b6b3-bc558f3a0969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response (Stop sequences; should stop before item 6):\n",
            "\n",
            "analysisUser wants \"10 safety tips for prompt engineering\" as numbered list. Provide concise, clear tips. Should be relevant to prompt engineering safety: avoid hallucinations, bias, misuse, privacy, etc. Provide numbered list 1-10.assistantfinal**10 Safety Tips for Prompt Engineering**\n",
            "\n",
            "1. **Start with a Clear Goal**  \n",
            "   Define the exact outcome you need before drafting the prompt. A vague objective leads to unpredictable or unsafe responses.\n",
            "\n",
            "2. **Use Explicit Constraints**  \n",
            "   Include boundaries in the prompt (e.g., “only use publicly available data,” “avoid personal identifiers,” “no political persuasion”). This helps the model stay within safe limits.\n",
            "\n",
            "3. **Avoid Ambiguous or Leading Language**  \n",
            "   Ambiguity can cause the model to hallucinate or generate unintended content. Phrase questions unambiguously and avoid loaded terms.\n",
            "\n",
            "4. **Limit Sensitive Content**  \n",
            "   Exclude or carefully handle topics that involve personal data, hate speech, or extremist content. If necessary, add a “do not generate” clause for disallowed content.\n",
            "\n",
            "5. **Iteratively Refine Prompts**  \n",
            "   Test the prompt on a small scale, review outputs, then adjust. Iteration catches issues early and reduces the risk of large‑scale misuse.\n",
            "\n",
            "6. **Implement Prompt Sanitization**  \n",
            "   Strip or mask user‑supplied input that could contain malicious code, injection attacks, or disallowed content before feeding it to the model.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Stop sequence example: cap a numbered list at 5\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  prompt_text = (\n",
        "    \"List 10 safety tips for prompt engineering as a numbered list.\"\n",
        "  )\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    temperature=0.5,\n",
        "    max_tokens=300,\n",
        "    stop = [\"\\n6.\", \"\\n 6.\", \"\\n**6.\", \"\\n6)\", \"\\n- 6.\"], # Sometimes it does not stop at bullet point number 6\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
        "  )\n",
        "  print(\"Response (Stop sequences; should stop before item 6):\\n\")\n",
        "  print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYyGylF5ijMG"
      },
      "source": [
        "## Mini Exercise\n",
        "- Rewrite one of your prompts using role prompting and a stop sequence.\n",
        "- Run it with `temperature=0.3` and `max_tokens<=200`.\n",
        "- Paste the result below and note what changed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain the C Programming Language in one short paragraph.\"\n",
        "\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    temperature=0.3,\n",
        "    max_tokens=200,\n",
        "    stop = [\"\\n1.\", \"\\n- \", \"\\n# \"], # Stop if it tries to start a list or new section\n",
        "    messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert programmer. Be direct and practical. Focus on what it is, what it does, and where it's used.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}]\n",
        "  )\n",
        "  print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0kHh5SpzJsq",
        "outputId": "5af5607f-9233-40d9-a330-c19c6e0629e0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analysisWe need to explain the C programming language in one short paragraph. The user wants a short paragraph. The developer instruction: \"Be direct and practical. Focus on what it is, what it does, and where it's used.\" So we should produce a concise paragraph, maybe 3-4 sentences. Let's do that.assistantfinalC is a general‑purpose, procedural programming language developed in the early 1970s by Dennis Ritchie. It provides low‑level access to memory, a compact syntax, and a rich set of operators, making it ideal for system software, embedded devices, and performance‑critical applications. C’s compilers translate code into efficient machine code, and its portability across hardware platforms has made it the foundation for operating systems (Unix, Linux), compilers, and many high‑level languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Result & What Changed:**\n",
        "\n",
        "Output from the web version of ChatGPT (for comparison):\n",
        "\n",
        "> C is a general-purpose, low-level programming language developed in the early 1970s for system programming, particularly implementing operating systems like UNIX. It provides direct access to memory through pointers, supports structured programming, and compiles efficiently to machine code, making it both powerful and performant. C is widely used in operating systems, embedded systems, compilers, databases, and performance-critical applications, and it remains foundational for modern languages like C++, Java, and Rust.\n",
        "\n",
        "\n",
        "Output from my engineered prompt (gpt-oss):\n",
        "\n",
        "> analysis: We need to produce a short paragraph explaining C. The user wants a short paragraph. The developer instruction: \"Be direct and practical. Focus on what it is, what it does, and where it's used.\" So we should give a concise paragraph.\n",
        "Let's craft: **\"C is a procedural programming language developed in the early 1970s by Dennis Ritchie at Bell Labs. It provides low-level access to memory, efficient execution, and a small set of keywords, making it ideal for system programming, embedded systems, operating systems, and performance-critical applications. Its portability and mature compiler ecosystem allow it to run on virtually any hardware, from microcontrollers to supercomputers.\" That is a short paragraph. Let's keep it concise.assistantfinalC is a procedural programming language created in the early 1970s by Dennis Ritchie at Bell Labs. It gives programmers low‑level memory access, efficient execution, and a small, expressive syntax, making it ideal**\n",
        "\n",
        "What Changed:\n",
        "\n",
        "- The \"Role\" Prompting: The AI clearly followed the system instruction to be an \"expert programmer.\" You can see it thinking about being \"direct and practical\" and focusing on \"what it is, what it does, and where it's used\" in its internal analysis.\n",
        "\n",
        "- The Low Temperature (0.3): The output is very deterministic and focused.\n",
        "\n",
        "- The Stop Sequence: The AI decided to write a paragraph, not a list, so the stop sequences (\\n1., \\n- ) were never needed.\n",
        "\n",
        "- The Max Tokens Limit Was Too Low: This was the biggest change. The model used its available 200 tokens to think through its answer (analysis...assistantfinal) instead of just giving the final answer. It ran out of tokens and was cut off before finishing its sentence."
      ],
      "metadata": {
        "id": "Xb20dSPx3FVG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFj6i2sj3uyP"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}