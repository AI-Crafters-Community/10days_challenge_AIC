{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe058aa2"
      },
      "source": [
        "# Prompt Engineering Guide\n",
        "Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently apply and build with large language models (LLMs) for a wide variety of applications and use cases.\n",
        "\n",
        "**Prompt engineering skills help to better understand the capabilities and limitations of LLMs. Researchers use prompt engineering to improve safety and the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.**\n",
        "\n",
        "This comprehensive guide covers the theory and practical aspects of prompt engineering and how to leverage the best prompting techniques to interact and build with LLMs.\n",
        "\n",
        "This guide will cover:\n",
        "\n",
        "*   Basic prompt structure\n",
        "*   Techniques for improving prompt effectiveness\n",
        "*   Examples of prompt engineering for different tasks\n",
        "*   Tips for debugging and refining prompts\n",
        "\n",
        "## Basic Prompt Structure\n",
        "\n",
        "A basic prompt often includes:\n",
        "\n",
        "1.  **Instruction:** What you want the model to do.\n",
        "2.  **Context (Optional):** Any background information the model needs.\n",
        "3.  **Input Data (Optional):** The specific data the model should process.\n",
        "4.  **Output Indicator (Optional):** How you want the output to be formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple: set API key for this session (hidden input)\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"API_KEY\"):\n",
        "  os.environ[\"API_KEY\"] = getpass(\"Enter your OpenRouter API key (hidden): \")\n",
        "print(\"API_KEY set:\", bool(os.getenv(\"API_KEY\")))\n",
        "\n",
        "# Alternative (one-liner): in a cell, run ->  %env API_KEY=sk-...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dotenv once (safe to re-run)\n",
        "%pip install -q python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: There was an error checking the latest version of pip.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43111466",
        "outputId": "c1f9fdcd-deec-4826-9da5-0423edc23b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.77.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\dell\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.0.4)\n",
            "Requirement already satisfied: certifi in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: There was an error checking the latest version of pip.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.77.0\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "print(openai.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bcedb8c",
        "outputId": "6621ca01-8848-4213-dfbf-c4f4c5f7aa04"
      },
      "outputs": [
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m load_dotenv(dotenv_path\u001b[38;5;241m=\u001b[39mdotenv_path, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m gpt_key\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://openrouter.ai/api/v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Example prompt\u001b[39;00m\n\u001b[0;32m     22\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a short story about a dog who loves to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\_client.py:116\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    114\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "# It's recommended to store your API key securely, for example, in Colab Secrets\n",
        "#from google.colab import userdata\n",
        "#gpt_key= userdata.get('gpt')\n",
        "\n",
        "\n",
        "\n",
        "dotenv_path = Path(r\"C:\\days\\.env\")  # point to the other folder\n",
        "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
        "\n",
        "gpt_key= os.getenv('OPENAI_API_KEY ')\n",
        "client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=gpt_key,\n",
        ")\n",
        "\n",
        "\n",
        "# Example prompt\n",
        "prompt_text = \"Write a short story about a dog who loves to read.\"\n",
        "\n",
        "# Example API call (using a model available on OpenRouter)\n",
        "# You can find available models and their names on the OpenRouter website\n",
        "model_name = \"openai/gpt-oss-20b:free\" # Example model, choose one from OpenRouter\n",
        "print(f\"Attempting to call model: {model_name}\")\n",
        "\n",
        "try:\n",
        "  chat_completion = client.chat.completions.create(\n",
        "  model=model_name,\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "  temperature=0.7,\n",
        "  max_tokens=1050,\n",
        "        )\n",
        "  print(\"Response from LLM:\")\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "  # Optionally, print the full object for debugging:\n",
        "  print(\"\\nFull chat_completion object:\")\n",
        "  print(chat_completion)\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during the API call: {e}\")\n",
        "  print(f\"Please ensure your API key is correct and the model '{model_name}' is valid and available on OpenRouter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G7EVmybsLSE",
        "outputId": "013bb923-1589-4cc8-be72-471e2c954b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The question “What is the meaning of life?” has occupied philosophers, theologians, scientists, artists, and ordinary people for millennia. Because there is no single, universally accepted answer, most traditions frame the question in terms of **how we choose to live** rather than “what” is the answer. Below are some of the major lenses through which people have tried to answer it, along with a few practical take‑aways that might help you find your own sense of purpose.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Religious and Spiritual Perspectives\n",
            "\n",
            "| Tradition | Core Idea | How It Shapes Meaning |\n",
            "|-----------|-----------|-----------------------|\n",
            "| **Christianity** | Life is a journey of love and service to God and others. | Purpose is found in following Jesus’ teachings, cultivating virtues, and contributing to the welfare of the community. |\n",
            "| **Islam** | Life is a test; success is measured by submission to Allah’s will. | Meaning comes from prayer, charity, living a life of moral integrity. |\n",
            "| **Buddhism** | Life is a cycle of suffering; liberation (nirvana) is the ultimate goal. | Purpose is to cultivate mindfulness, compassion, and insight to escape the cycle of reb. |\n",
            "| **Hinduism** | Dharma (duty), artha (prosperity), kama (desire), and moksha (liberation). | Life’s meaning blends personal fulfillment, social responsibility, and spiritual liberation. |\n",
            "| **Secular Humanism** | No divine plan; meaning is created through human relationships and ethical living. | Purpose is derived from empathy, intellectual curiosity, and the betterment of society. |\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Philosophical Perspectives\n",
            "\n",
            "| School | Core Idea | How It Shapes Meaning |\n",
            "|--------|-----------|-----------------------|\n",
            "| **Existentialism** | Life has no inherent meaning; we create it through choices. | Authentic living means embracing freedom, responsibility, and the inevitability of death. |\n",
            "| **Absurdism** | The human desire for meaning clashes with a silent universe. | Meaning is found in the defiant joy of living, regardless of cosmic indifference. |\n",
            "| **Nihilism** | Life has no objective value or purpose. | Some nihilists embrace the freedom from moral constraints; others seek personal meaning anyway. |\n",
            "| **Stoicism** | Focus on what you can control; accept what you cannot. | Purpose is living in accordance with nature’s rational order, cultivating inner peace. |\n",
            "| **Utilitarianism** | Maximize happiness for the greatest number. | Meaning is measured by the positive impact one has on others’ well‑being. |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Scientific & Evolutionary Views\n",
            "\n",
            "- **Biological Perspective**: Life’s “goal” is to survive and reproduce. From this viewpoint, meaning is an evolutionary by‑product of the brain’s reward system, which associates pleasurable experiences with survival‑enhancing behaviors.\n",
            "- **Cosmological\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=gpt_key,\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  extra_body={},\n",
        "  model=\"openai/gpt-oss-20b:free\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=1000,\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What is the meaning of life?\"\n",
        "    }\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFp1zhgyHbwR"
      },
      "source": [
        "## LLM Settings\n",
        "* **Temperature** → controls randomness of next-token choice.\n",
        "\n",
        "  * Lower (0–0.3): more deterministic, concise (good for fact Q&A).\n",
        "\n",
        "  * Higher (0.7–1.0+): more varied/creative (good for poems/brainstorm).\n",
        "\n",
        "* **Top-p** (nucleus sampling) → limits choices to the smallest set whose probabilities sum to p.\n",
        "\n",
        "  * Lower (0.1–0.3): very focused, conservative.\n",
        "\n",
        "  * Higher (0.9–1.0): considers more (rarer) words → more diversity.\n",
        "\n",
        "      * Tip: tune either temperature or top-p, not both.\n",
        "\n",
        "* **Max length** (max tokens) → hard cap on output size.\n",
        "\n",
        "  * Prevents run-on/irrelevant answers and manages cost.\n",
        "\n",
        "  * Stop sequences → strings that make the model stop generating.\n",
        "\n",
        "  * Use to enforce structure/length (e.g., stop at \"11\" to cap a numbered list at 10).\n",
        "\n",
        "* **Frequency penalty** → reduces reuse proportional to how often a token already appeared.\n",
        "\n",
        "  * Higher value = fewer repeated words.\n",
        "\n",
        "  * Presence penalty → discourages any repeated token equally (once it’s appeared).\n",
        "\n",
        "  * Higher value = less phrase repetition; boosts topical variety.\n",
        "\n",
        "  * Use higher for exploration/creativity; lower to keep the model tightly on topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "319fba6b"
      },
      "source": [
        "## Prompt Examples\n",
        "Here are examples of Zero-shot, One-shot, and Few-shot prompting techniques:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iqh59uCEFPs"
      },
      "source": [
        "### Zero-shot Prompting\n",
        "\n",
        "Zero-shot prompting is when you give the model a task without providing any examples of how to do it. The model relies solely on its pre-training to understand and complete the task.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23e5bebd",
        "outputId": "a28cea84-98a7-462d-fc3d-a32dd8778af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot Prompt:\n",
            "Classify the following text as positive, negative, or neutral: 'I love this new movie!'\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot Prompt Example\n",
        "zero_shot_prompt = \"Classify the following text as positive, negative, or neutral: 'I love this new movie!'\"\n",
        "print(f\"Zero-shot Prompt:\\n{zero_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLCNuCD0Dk1L"
      },
      "source": [
        "### One-shot Prompting\n",
        "One-shot prompting is when you provide the model with one example of the task you want it to perform, along with the desired output for that example. This helps the model understand the format and style you're looking for in its response to your actual query.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6136412",
        "outputId": "6de8cc1a-9a79-45a6-877d-cfe21daa9df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-shot Prompt:\n",
            "Given the following example, classify the next text.\n",
            "\n",
            "Example:\n",
            "Text: 'This is a terrible product.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'The weather is nice today.'\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "# One-shot Prompt Example\n",
        "one_shot_prompt = \"\"\"Given the following example, classify the next text.\n",
        "\n",
        "Example:\n",
        "Text: 'This is a terrible product.'\n",
        "Sentiment: Negative\n",
        "\n",
        "Classify the following text:\n",
        "Text: 'The weather is nice today.'\n",
        "Sentiment:\"\"\"\n",
        "print(f\"One-shot Prompt:\\n{one_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycp2jX3yDvzm"
      },
      "source": [
        "### Few-shot prompting\n",
        "Few-shot prompting is similar to one-shot prompting, but instead of just one example, you provide the model with a few examples (typically between 2 and 5) of the task and their corresponding outputs. This gives the model more context and helps it to better grasp the pattern or logic required to complete the task accurately.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b58601f",
        "outputId": "0a28cac2-76e2-458d-d9f7-36fc98b08369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot Prompt:\n",
            "Given the following examples, classify the next text.\n",
            "\n",
            "Example 1:\n",
            "Text: 'I had a wonderful time.'\n",
            "Sentiment: Positive\n",
            "\n",
            "Example 2:\n",
            "Text: 'The service was slow.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Example 3:\n",
            "Text: 'It was an average experience.'\n",
            "Sentiment: Neutral\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'I would recommend this restaurant.'\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "# Few-shot Prompt Example\n",
        "few_shot_prompt = \"\"\"Given the following examples, classify the next text.\n",
        "\n",
        "Example 1:\n",
        "Text: 'I had a wonderful time.'\n",
        "Sentiment: Positive\n",
        "\n",
        "Example 2:\n",
        "Text: 'The service was slow.'\n",
        "Sentiment: Negative\n",
        "\n",
        "Example 3:\n",
        "Text: 'It was an average experience.'\n",
        "Sentiment: Neutral\n",
        "\n",
        "Classify the following text:\n",
        "Text: 'I would recommend this restaurant.'\n",
        "Sentiment:\"\"\"\n",
        "print(f\"Few-shot Prompt:\\n{few_shot_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Xhv8cIEh_N",
        "outputId": "0e8c9038-e427-4ae5-ae34-57e5e2564e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sending Zero-shot Prompt ---\n",
            "Prompt:\n",
            "Classify the following text as positive, negative, or neutral: 'I love this new movie!'\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (Zero-shot):\n",
            "positive\n",
            "\n",
            "--- Sending One-shot Prompt ---\n",
            "Prompt:\n",
            "Given the following example, classify the next text.\n",
            "\n",
            "Example:\n",
            "Text: 'This is a terrible product.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'The weather is nice today.'\n",
            "Sentiment:\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (One-shot):\n",
            "Sentiment: Positive\n",
            "\n",
            "--- Sending Few-shot Prompt ---\n",
            "Prompt:\n",
            "Given the following examples, classify the next text.\n",
            "\n",
            "Example 1:\n",
            "Text: 'I had a wonderful time.'\n",
            "Sentiment: Positive\n",
            "\n",
            "Example 2:\n",
            "Text: 'The service was slow.'\n",
            "Sentiment: Negative\n",
            "\n",
            "Example 3:\n",
            "Text: 'It was an average experience.'\n",
            "Sentiment: Neutral\n",
            "\n",
            "Classify the following text:\n",
            "Text: 'I would recommend this restaurant.'\n",
            "Sentiment:\n",
            "Attempting to call model: openai/gpt-oss-20b:free\n",
            "Response from LLM (Few-shot):\n",
            "Sentiment: Positive\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'client' is already defined and configured from a previous cell\n",
        "# and 'zero_shot_prompt', 'one_shot_prompt', and 'few_shot_prompt' are defined\n",
        "\n",
        "model_name = \"openai/gpt-oss-20b:free\" # Example model, choose one from OpenRouter\n",
        "\n",
        "prompts = {\n",
        "    \"Zero-shot\": zero_shot_prompt,\n",
        "    \"One-shot\": one_shot_prompt,\n",
        "    \"Few-shot\": few_shot_prompt\n",
        "}\n",
        "\n",
        "for prompt_type, prompt_text in prompts.items():\n",
        "  print(f\"\\n--- Sending {prompt_type} Prompt ---\")\n",
        "  print(f\"Prompt:\\n{prompt_text}\")\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "    temperature=0.7,\n",
        "    max_tokens=1050,\n",
        "          )\n",
        "    print(f\"Response from LLM ({prompt_type}):\")\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "    # Optionally, print the full object for debugging:\n",
        "    # print(f\"\\nFull chat_completion object ({prompt_type}):\")\n",
        "    # print(chat_completion)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during the API call for {prompt_type} prompt: {e}\")\n",
        "    print(f\"Please ensure your API key is correct and the model '{model_name}' is valid and available on OpenRouter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h08c6_DRE6iW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b66a625"
      },
      "source": [
        "## Chain-of-Thought Prompting\n",
        "\n",
        "Chain-of-Thought (CoT) prompting is a technique that encourages the language model to explain its reasoning process step-by-step before arriving at the final answer. This can lead to more accurate and reliable results, especially for complex tasks, as it allows the model to break down the problem and work through it logically.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "By adding phrases like \"Let's think step by step\" or explicitly asking for intermediate steps, you guide the model to generate a series of thoughts or reasoning steps that lead to the solution.\n",
        "\n",
        "**Benefits of CoT:**\n",
        "\n",
        "* **Improved accuracy:** Breaking down complex problems into smaller steps reduces the chance of errors.\n",
        "* **Increased transparency:** You can see how the model arrived at its answer, making it easier to debug or understand its limitations.\n",
        "* **Better performance on complex tasks:** CoT has shown significant improvements on tasks requiring multi-step reasoning, like arithmetic or symbolic manipulation.\n",
        "\n",
        "**Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af5d430e"
      },
      "outputs": [],
      "source": [
        "# Chain-of-Thought Prompt Example\n",
        "cot_prompt = \"\"\"The original price of a shirt was $20. It was discounted by 25%, and then an additional 10% discount was applied to the sale price. What is the final price of the shirt?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "print(f\"Chain-of-Thought Prompt:\\n{cot_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send the Chain-of-Thought prompt to the LLM and print the response\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "  chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0.2,\n",
        "    max_tokens=300\n",
        "  )\n",
        "  print(\"Response (Chain-of-Thought):\\n\")\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred during the API call: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## System / Role Prompting\n",
        "\n",
        "You can steer behavior with explicit roles:\n",
        "\n",
        "- **system**: high-level instructions and persona\n",
        "- **user**: task request\n",
        "- **assistant**: optional exemplars or tools output\n",
        "\n",
        "This helps keep outputs consistent, especially for style and safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System / Role Prompting demo\n",
        "system_msg = (\n",
        "  \"You are a concise technical writer. Answer in bullet points, each under 12 words.\"\n",
        ")\n",
        "user_msg = \"Explain what embeddings are and one practical use-case.\"\n",
        "\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  print(f\"Attempting to call model: {model_name}\")\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    temperature=0.4,\n",
        "    max_tokens=200,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": system_msg},\n",
        "      {\"role\": \"user\", \"content\": user_msg}\n",
        "    ]\n",
        "  )\n",
        "  print(\"Response (System/Role Prompting):\\n\")\n",
        "  print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Stop Sequences\n",
        "Use stop strings to end generation at desired boundaries (e.g., JSON blocks, section breaks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop sequence example: cap a numbered list at 5\n",
        "try:\n",
        "  model_name = \"openai/gpt-oss-20b:free\"\n",
        "  prompt_text = (\n",
        "    \"List 10 safety tips for prompt engineering as a numbered list.\"\n",
        "  )\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    temperature=0.5,\n",
        "    max_tokens=300,\n",
        "    stop=[\"6.\", \"\\n6)\"],\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
        "  )\n",
        "  print(\"Response (Stop sequences; should stop before item 6):\\n\")\n",
        "  print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Mini Exercise\n",
        "- Rewrite one of your prompts using role prompting and a stop sequence.\n",
        "- Run it with `temperature=0.3` and `max_tokens<=200`.\n",
        "- Paste the result below and note what changed.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
